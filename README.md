# Linear Regression Example

*The data and its analysis are the scientific product. The paper is just an advertisement.*
---Richard McElreath

### Objective

Learn how to develop your research using a reproducible workflow.

### To-Do

In this example, you will implement the following methods in `linear_regression.py` as well as tests for each method in `test_linear_regression.py`:

+ `simulate_data()`
+ `compare_models()`
+ `load_hospital_data()`
+ `prepare_data()`
+ `run_hospital_regression()`

**Let's Begin!**

Suppose you want to run a regression or a series of regressions on a dataset about hospital charge data found online at data.gov/health. There are a number of questions you might want to ask this dataset. For this example, suppose you are interested in whether the amount that a hospital bills Medicare has an effect on the amount actually paid out by Medicare.

*Simulating Data*

A really good practice is to create a simulated dataset of the kind of data you will be working with. This has a number of advantages. First, the simulation is completely distinct from the actual dataset and avoids concerns about private or proprietary data. Second, it helps you think critically about the data generating process (how the data came to be). This in turn helps you select an appropriate estimator because you know what the assumptions are about the data; you explicitly laid them out by simulating the dataset.

To begin, assume that the variable `x1` represents the amount of money that is billed to Medicare by a certain provider on average. The name given for this variable in the hospital charges dataset is `average covered charges`. Suppose `x1` is exponentially distributed with rate parameter 9,000. You can change this at any time during the analysis to see what happens to the estimates if the distribution isn't quite what you expected. Now suppose there is another variable `x2` that you want to control for; it represents the number of hospital discharges associated with the given time period. Let it be poisson distributed with the rate parameter at 15. 

Next generate a vector of parameters `beta` that will represent the coefficients you want to estimate. In real world data sets, these parameters are never observed so it can be hard to know whether your method worked or not. While you can check all of your assumptions manually, it gives you more confidence if you can generate simulated data from a set of parameters and be able to check your parameter estimates against the simulated 'true' parameters.

Finally, the response variable `y` is generated by multiplying the matrix `X` (containing `x1` and `x2`) by the vector `beta` and adding a noise term `epsilon` which is always assumed to come from a zero-mean normal distribution.

*Testing Methods on Simulated Data*

Now that you can create simulated data, try writing a method to see if you can correctly estimate the true parameter values. Compare the implementations in two Python packages: `statsmodels` and `sklearn`. Both methods should produce identical estimates of the parameters. (Hint: this might make for a good unit test.)

One nice thing about statistical estimators is that they come with some guarantees, as long as all the assumptions are met sufficiently. This means that you can write tests to make sure that your estimator passes these guarantees. For example, you can check to see that the estimated parameters are within some tolerance of the true paramter values. So long as these tests pass, it indicates that the estimator is likely good.

*Preparing the Hospital Charges Dataset*

Now that you have validated your methods and know how to use each implementation in each package, you can start working with real data. The hospital charge data found at data.gov is not particularly large but it's not small either. It is not good practice to store large datasets on github. To get around this, it can sometimes be nice to sample observations from the large dataset and use that in continuing your analysis. A sample from the hospital charges dataset is found in the file `hospital_charge_sample.csv`. You will use this smaller file when writing methods to prepare the data for analysis.

Loading the data may seem straitforward, but when the data is raw it may need to go through some processing before it can be prepared for regression. One good practice is to keep the raw data and write functions that 'clean' the dataset. This may not be possible for all applications but it is good practice when it is possible and avoids many accidental errors. It also keeps a log (in code form) of how the data was changed so that you know what you're working with. In this example, each row in the dataset should correspond to the `provider id` in the original hosptial charges dataset. This means that you will have to aggregate some variables such as `total discharges`.

Once the data is cleaned, meaning it follows standard conventions of a dataset (e.g. Hadley Wickham's TIDY format) and can be easily loaded and worked with, then it is time to write a method that prepares the data for the linear regression model. This may involve normalizing or log-transforming variables or creating new variables from linear combinations of others. In this case, just like when you simulated data, you will use the variables `average covered charges` and `total discharges` from the dataset. You can decide how this data needs to enter the regression.

*Run the Regression*

Finally, it's time to write a method that runs the regression for you. This method should involve the methods that clean the data and prepare the data for the regression. In this case, the output should come from the `statsmodels` package and should return the output table in text form as shown below. As soon as everything works and all tests pass, you can have complete confidence that when you use the full dataset, everything will work as expected. Once you have the simple model working, you can begin to experiment or add complexity to the analysis.


```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.869
Model:                            OLS   Adj. R-squared:                  0.866
Method:                 Least Squares   F-statistic:                     425.4
Date:                Wed, 18 Sep 2019   Prob (F-statistic):               0.00
Time:                        12:51:02   Log-Likelihood:                -2524.2
No. Observations:                3337   AIC:                             5152.
Df Residuals:                    3285   BIC:                             5470.
Df Model:                          51                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          6.6815      0.051    130.174      0.000       6.581       6.782
x1             0.9762      0.007    135.059      0.000       0.962       0.990
x2             0.4945      0.171      2.899      0.004       0.160       0.829
x3            -0.0466      0.055     -0.852      0.394      -0.154       0.061
x4            -0.1323      0.077     -1.712      0.087      -0.284       0.019
x5             0.4377      0.067      6.558      0.000       0.307       0.569
x6             0.9443      0.033     28.899      0.000       0.880       1.008
x7             0.6392      0.077      8.273      0.000       0.488       0.791
x8            -0.0017      0.093     -0.019      0.985      -0.184       0.180
x9             0.4286      0.193      2.218      0.027       0.050       0.807
x10           -0.0818      0.209     -0.392      0.695      -0.491       0.327
x11            0.4702      0.042     11.078      0.000       0.387       0.553
x12            0.1698      0.051      3.315      0.001       0.069       0.270
x13            0.6739      0.137      4.920      0.000       0.405       0.942
x14           -0.0810      0.089     -0.915      0.360      -0.255       0.093
x15            0.2382      0.142      1.677      0.094      -0.040       0.517
x16            0.1814      0.048      3.819      0.000       0.088       0.275
x17            0.0930      0.056      1.647      0.100      -0.018       0.204
x18            0.0334      0.071      0.471      0.638      -0.106       0.172
x19           -0.1547      0.065     -2.387      0.017      -0.282      -0.028
x20            0.2252      0.056      4.020      0.000       0.115       0.335
x21           -0.5078      0.068     -7.515      0.000      -0.640      -0.375
x22           -0.8493      0.078    -10.931      0.000      -1.002      -0.697
x23            0.0376      0.112      0.336      0.737      -0.182       0.257
x24           -0.2755      0.055     -4.996      0.000      -0.384      -0.167
x25            0.0161      0.073      0.219      0.827      -0.128       0.160
x26            0.0721      0.061      1.190      0.234      -0.047       0.191
x27           -0.0034      0.067     -0.050      0.960      -0.134       0.128
x28           -0.0789      0.142     -0.555      0.579      -0.357       0.200
x29           -0.1733      0.056     -3.078      0.002      -0.284      -0.063
x30           -0.0726      0.193     -0.376      0.707      -0.451       0.306
x31            0.1174      0.107      1.095      0.274      -0.093       0.328
x32            0.1922      0.142      1.352      0.176      -0.086       0.471
x33            0.7652      0.066     11.622      0.000       0.636       0.894
x34            0.2466      0.091      2.698      0.007       0.067       0.426
x35            0.8890      0.112      7.929      0.000       0.669       1.109
x36           -0.0387      0.043     -0.903      0.366      -0.123       0.045
x37           -0.0263      0.047     -0.563      0.573      -0.118       0.065
x38           -0.0157      0.057     -0.275      0.784      -0.128       0.097
x39            0.3351      0.091      3.675      0.000       0.156       0.514
x40            0.1583      0.044      3.611      0.000       0.072       0.244
x41            0.2463      0.154      1.596      0.111      -0.056       0.549
x42            0.2339      0.070      3.362      0.001       0.097       0.370
x43           -0.0277      0.121     -0.229      0.819      -0.265       0.210
x44           -0.0011      0.054     -0.021      0.983      -0.106       0.104
x45            0.3790      0.032     11.734      0.000       0.316       0.442
x46            0.1395      0.096      1.454      0.146      -0.049       0.328
x47           -0.0095      0.060     -0.158      0.874      -0.128       0.109
x48           -0.1762      0.208     -0.845      0.398      -0.585       0.233
x49            0.3126      0.076      4.126      0.000       0.164       0.461
x50            0.1096      0.065      1.693      0.091      -0.017       0.237
x51           -0.2863      0.091     -3.140      0.002      -0.465      -0.108
x52            0.4421      0.154      2.862      0.004       0.139       0.745
==============================================================================
Omnibus:                      943.561   Durbin-Watson:                   1.953
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8485.047
Skew:                          -1.082   Prob(JB):                         0.00
Kurtosis:                      10.506   Cond. No.                     1.16e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 1.29e-29. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
```


Congratulations! You have completed this exercise using a fully reproducible workflow. As you continue to learn more, you will find other ways to continue making your research more reproducible. This helps collaborators and other colleagues understand your work and gives you more confidence that your work is correct and accurate.
